{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f68276-a7e2-43e8-8eb0-191ccefa7ea1",
   "metadata": {},
   "source": [
    "# XGBoost Algorithm Overview\n",
    "\n",
    "XGBoost, or Extreme Gradient Boosting, is an efficient, scalable machine learning algorithm used primarily for supervised learning tasks like classification and regression. It builds upon gradient boosting principles to create an ensemble of weak learners that sequentially correct the errors of previous models to improve accuracy.\n",
    "\n",
    "## How XGBoost Works\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Starts with an initial prediction (average value for regression or a default probability for classification).\n",
    "\n",
    "2. **Iterative Model Training**:\n",
    "   - In each step, a new weak learner (we are using decision tree) is trained to minimize residual errors from previous models. \n",
    "   - The weak learner is trained on a modified dataset where the target is now the residual error from the last iteration.\n",
    "\n",
    "3. **Gradient Boosting with Regularization**:\n",
    "   \n",
    "   - XGBoost includes regularization terms in the objective function to control overfitting:\n",
    "     $$\n",
    "     \\text{Objective} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(h_k)\n",
    "     $$\n",
    "     where $L(y_i, \\hat{y}_i)$ is the loss function, $\\Omega(h_k)$ is the regularization term for tree $h_k$, $n$ is the numble of samples, and $k$ is the number of trees (Chen & Guestrin, 2016). \n",
    "   \n",
    "4. **Shrinking (Learning Rate)**:\n",
    "   - Applies a learning rate to scale each weak learner’s contribution, ensuring gradual model improvement to prevent overfitting.\n",
    "   \n",
    "5. **Tree Pruning**:\n",
    "   - Uses constraints like \"max depth\" to limit tree depth, preventing overfitting.\n",
    "   \n",
    "6. **Weighted Data and Column Sampling**:\n",
    "   - Row and column sampling prevent overfitting, making the model more robust to noisy data.\n",
    "   \n",
    "7. **Final Prediction**:\n",
    "   - Predictions are generated by aggregating the outputs of all weak learners, often by summing their outputs.\n",
    "\n",
    "## Advantages of XGBoost\n",
    "\n",
    "1. **Highly Efficient and Scalable**:\n",
    "   - Optimized for speed, utilizing CPU/GPU resources for large datasets.\n",
    "\n",
    "2. **Regularization**:\n",
    "   - L1 and L2 regularization helps reduce overfitting, improving generalization (Friedman, 2001).\n",
    "\n",
    "3. **Custom Loss Functions**:\n",
    "   - Allows custom loss functions, adapting well to various tasks and metrics.\n",
    "\n",
    "4. **Handles Missing Values**:\n",
    "   - Automatically learns the best direction for missing values during training.\n",
    "\n",
    "5. **Parallel and Distributed Computing**:\n",
    "   - Supports parallel tree boosting, and distributed training, making it suitable for very large datasets.\n",
    "\n",
    "6. **Feature Importance and Interpretability**:\n",
    "   - Provides feature importance scores for insight into feature contributions.\n",
    "\n",
    "## Disadvantages of XGBoost\n",
    "\n",
    "1. **Complexity in Tuning**:\n",
    "   - Many hyperparameters require tuning; poor parameter settings may lead to suboptimal performance.\n",
    "\n",
    "2. **Sensitive to Noise**:\n",
    "   - Can overfit noisy data or when trees are too deep, despite regularization.\n",
    "\n",
    "3. **High Memory Consumption**:\n",
    "   - Memory-intensive on large datasets with high-dimensional data.\n",
    "\n",
    "4. **Not Ideal for Small Datasets**:\n",
    "   - On small datasets, simpler models may perform better with fewer resources.\n",
    "\n",
    "5. **Black-box Nature**:\n",
    "   - Though feature importance scores provide some interpretability, XGBoost can still be difficult to fully interpret.\n",
    "\n",
    "## Summary\n",
    "\n",
    "XGBoost is a powerful algorithm, ideal for large datasets and structured data tasks, but its complexity and memory requirements can make it less suitable for small datasets or when a highly interpretable model is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b176e44a-b13e-4ebd-bb2f-e1162675dc44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Representation of XGBoost\n",
    "\n",
    "In XGBoost with Decision Tree as the weak learner, predictions are made by combining the outputs of a sequence of decision trees. Here’s how XGBoost generates a single prediction from feature values:\n",
    "\n",
    "1. **Initialization**:\n",
    "\n",
    "   - The model starts with an initial prediction for all samples, often set to zero or the average target value if it's a regression task. Let's denote this initial prediction as $F^{(0)}(x)$.\n",
    "\n",
    "2. **Training Decision Trees**:\n",
    "\n",
    "   - In each boosting round $t$, a new Decision Tree $h_t(x)$ is trained to predict the residuals (the difference between the true values $y_i$ and the current predictions $F^{(t-1)}(x_i)$.\n",
    "   - At each split, the Decision Tree splits the data based on a single feature and threshold, recursively creating a set of rules. The final prediction for each sample is determined by the leaf node it falls into after traversing the tree.\n",
    "\n",
    "3. **Tree Prediction**:\n",
    "\n",
    "   - For a feature $x$ and a threshold $\\theta$, a single split in the Decision Tree assigns predictions to samples based on the threshold: \n",
    "     $$\n",
    "     h_t(x) = \\begin{cases} \n",
    "           y_{\\text{left}} & \\text{if } x_i < \\theta \\\\\n",
    "           y_{\\text{right}} & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "     $$\n",
    "\n",
    "   - Here, $y_{\\text{left}}$ and $y_{\\text{right}}$ represent the predicted values for the samples on each side of the split. These predictions are often chosen to minimize the overall error in the objective function (Hastie, Tibshirani & Friedman, 2009).\n",
    "\n",
    "4. **Updating the Overall Prediction**:\n",
    "\n",
    "   - The model’s prediction is updated by adding a scaled version of the Decision Tree’s prediction. The learning rate $\\eta$ controls how much each tree contributes to the final model: \n",
    "     $$\n",
    "     F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)\n",
    "     $$\n",
    "\n",
    "   - This update means each Decision Tree contributes only a small correction to the existing prediction, allowing the model to make gradual adjustments rather than large changes.\n",
    "\n",
    "5. **Final Prediction**:\n",
    "\n",
    "   - After $T$ boosting rounds, the final prediction for a data point $x$ is the sum of all weak learners' contributions: \n",
    "     $$\n",
    "     F(x) = \\sum_{t=1}^T \\eta h_t(x)\n",
    "     $$\n",
    "\n",
    "   - Each Decision Tree captures patterns by recursively splitting data based on features and thresholds. By combining multiple trees, XGBoost can approximate complex relationships in the data (Chen & Guestrin, 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c26193-0857-4fd3-a921-636b20bc8ad5",
   "metadata": {},
   "source": [
    "# Loss of XGBoost\n",
    "\n",
    "The loss is used to measure the error between predicted and actual values. XGBoost supports various loss functions tailored to different types of tasks, including regression and classification tasks.\n",
    "\n",
    "1. **Loss Function**:\n",
    "    For the regression task, we can use Mean Squared Error(MSE) or Mean Absolute Error (MAE) (Hastie, Tibshirani & Friedman, 2009)\n",
    "\n",
    "    Mean Squared Error(MSE):\n",
    "    $$\n",
    "        L(F^{\\mathbf{(t)}}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - F^{\\mathbf{(t)}}(\\mathbf{x}_i))^2\n",
    "    $$\n",
    "\n",
    "    Mean Absolute Error (MAE):\n",
    "    $$\n",
    "        L(F^{\\mathbf{(t)}}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - F^{\\mathbf{(t)}}(\\mathbf{x}_i)|\n",
    "    $$\n",
    "    \n",
    "    For the binary classification task, we can use Binary Cross Entropy Loss\n",
    "    Binary Cross Entropy Loss:\n",
    "\n",
    "    $$\n",
    "    L_S(F^{\\mathbf{(t)}}) = -\\frac{1}{n} \\sum_{i=1}^{n}\\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - F^{\\mathbf{(t)}}(\\mathbf{x}_i)) \\right]\n",
    "    $$\n",
    "\n",
    "    For the multiclass classification task, we can use Cross Entropy Loss\n",
    "    Cross Entropy Loss (Bishop, 2006):\n",
    "    $$\n",
    "        L_S(F^{\\mathbf{(t)}}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} \\mathbb{1}[y_i = j] \\log F^{\\mathbf{(t)}}(\\mathbf{x}_i)_j\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    \n",
    "    - $y_i$ is the $i$-th actual value \n",
    "\n",
    "    - $n$ is the number of samples \n",
    "\n",
    "    - $k$ is the number of classes \n",
    "    \n",
    "    - $j$ is the $j$-th class \n",
    "    \n",
    "    - $F^{\\mathbf{(t)}}$ is the model at the $t$-th iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ecff9-3ad4-4921-b638-01288b993ea7",
   "metadata": {},
   "source": [
    "# XGBoost with Decision Tree as Weak Learner: Optimizer Update\n",
    "\n",
    "In this configuration, XGBoost uses a decision tree as the weak learner. The optimizer is updated to account for the decision tree mechanism, which recursively splits the data based on features and thresholds. Each leaf node assigns a constant value to the samples it contains. The prediction update process incorporates a learning rate $\\eta$ to scale the contribution of each tree, ensuring gradual and controlled adjustments to the model (Quinlan, 1996).\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "The objective function consists of the loss and regularization terms:\n",
    "\n",
    "$$\n",
    "\\text{Objective} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(h_k)\n",
    "$$\n",
    "\n",
    "where $L(y_i, \\hat{y}_i)$ is the loss function, typically squared error or logistic loss, measuring the difference between the true values $y_i$ and predictions $\\hat{y}_i$. $\\Omega(h_k)$ is the regularization term to control model complexity.\n",
    "\n",
    "At each iteration $t$, the model updates the prediction with the new decision tree’s prediction, scaled by the learning rate $\\eta$:\n",
    "\n",
    "$$\n",
    "F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)\n",
    "$$\n",
    "\n",
    "where $h_t(x)$ represents the decision tree’s prediction.\n",
    "\n",
    "## Weak Learner\n",
    "\n",
    "For a feature $x$ and a threshold $\\theta$, a single split in the Decision Tree assigns predictions to samples based on the threshold: \n",
    "     $$\n",
    "     h_t(x) = \\begin{cases} \n",
    "           y_{\\text{left}} & \\text{if } x_i < \\theta \\\\\n",
    "           y_{\\text{right}} & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "     $$\n",
    "\n",
    "Here, $y_{\\text{left}}$ and $y_{\\text{right}}$ represent the predicted values for the samples on each side of the split. These predictions are often chosen to minimize the overall error in the objective function.\n",
    "\n",
    "## Approximation with Taylor Expansion\n",
    "\n",
    "To facilitate optimization, we apply a second-order Taylor expansion around the current prediction $F^{(t-1)}$ to approximate the loss function $L(F^{(t)})$:\n",
    "\n",
    "$$\n",
    "L(F^{(t)}) \\approx \\sum_{i=1}^{n} \\left[ L(y_i, F^{(t-1)}(x_i)) + g_i h_t(x_i) + \\frac{1}{2} h_i h_t(x_i)^2 \\right] + \\Omega(h_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $g_i = \\frac{\\partial L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)}$ is the first derivative of the loss with respect to the previous prediction (the gradient).\n",
    "- $h_i = \\frac{\\partial^2 L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)^2}$ is the second derivative (the Hessian).\n",
    "\n",
    "## Regularization and Optimal Leaf Weights\n",
    "\n",
    "The regularization term for a decision tree $\\Omega(h_t)$ is given by:\n",
    "\n",
    "$$\n",
    "\\Omega(h_t) = \\gamma N + \\frac{1}{2} \\lambda \\sum_{j=1}^{N} w_j^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of leaf nodes,\n",
    "- $w_j$ is the weight assigned to each leaf, \n",
    "- $\\gamma$ controls the complexity penalty, and $\\lambda$ controls the weight shrinkage.\n",
    "\n",
    "The optimal weight for each leaf $j$ is obtained by minimizing the regularized objective:\n",
    "\n",
    "$$\n",
    "w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}\n",
    "$$\n",
    "\n",
    "where $I_j$ is the set of sample indices for leaf $j$.\n",
    "\n",
    "## Gain Calculation and Tree Update\n",
    "\n",
    "The gain for adding a new tree, which represents the improvement in the objective function, is:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\sum_{j=1}^{N} \\frac{\\left( \\sum_{i \\in I_j} g_i \\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} - \\gamma N\n",
    "$$\n",
    "\n",
    "This gain metric helps determine the best split points and decide whether further splitting is beneficial.\n",
    "\n",
    "## Prediction Update\n",
    "\n",
    "Finally, the model’s prediction is updated at each iteration with the contribution from the newly added decision tree:\n",
    "\n",
    "$$\n",
    "F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b4b0e-a568-4339-b914-b4ae53a8746b",
   "metadata": {},
   "source": [
    "# XGBoost Pseudo-code\n",
    "\n",
    "**Input**:  \n",
    "- Training set $S = \\{(x_1, y_1), \\ldots, (x_m, y_m)\\}$  \n",
    "- Weak learner $h_t(x)$ (Decision Tree)  \n",
    "- Number of boosting rounds $T$  \n",
    "- Learning rate $\\eta$  \n",
    "- Regularization parameters $\\lambda, \\gamma$  \n",
    "\n",
    "**Initialize**:  \n",
    "$F^{(0)}(x) = 0$  \n",
    "\n",
    "**for** $t = 1, \\ldots, T$:  \n",
    "1. **Compute gradients and Hessians**:  \n",
    "   $g_i = \\frac{\\partial L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)}$  \n",
    "   $h_i = \\frac{\\partial^2 L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)^2}$  \n",
    "\n",
    "2. **Find the best split**:  \n",
    "   - For each feature and threshold $\\theta$:  \n",
    "     - Split data into left and right groups based on $\\theta$  \n",
    "     - Compute split gain using gradients and Hessians  \n",
    "   - Select feature and threshold $\\theta$ with the highest gain  \n",
    "\n",
    "3. **Train decision tree** $h_t(x)$:  \n",
    "   - Fit $h_t(x)$ using the selected splits  \n",
    "   - Assign values $y_{\\text{left}}$ and $y_{\\text{right}}$ to leaf nodes  \n",
    "   - Compute optimal weights $w_j$ for each leaf node  \n",
    "\n",
    "4. **Update model**:  \n",
    "   $F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)$  \n",
    "\n",
    "**Output**:  \n",
    "Final model prediction: $F(x) = F^{(T)}(x)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb6fd4-419f-496e-9622-bf0aae0585a1",
   "metadata": {},
   "source": [
    "# Comparisons of Previous Works Using XGBoost\n",
    "\n",
    "**Introduction of Python XBGoost Module**:\n",
    "\n",
    "The XGBoost Python module is the implementation of the XGBoost algorithm in Python. This module supports loading datasets and training models using the sklearn estimator interface from the sklearn module. With this interface, users can set the loss function based on the task and choose the type of tree used in XGBoost. This allows XGBoost to make predictions on different datasets. The previous works we selected all used this module for training and prediction on different datasets.\n",
    "\n",
    "**Breast Cancer Diagnosis**:\n",
    "\n",
    "This work uses XGBoost to diagnose whether a breast mass is benign or malignant. The dataset used is the Breast Cancer Wisconsin (Diagnostic) Data Set. The features of this dataset are derived from a digitized image of a fine needle aspirate (FNA) of a breast mass. Below is a list of features:\n",
    "\n",
    "| **Feature**             | **Description**                                                        |\n",
    "|--------------------------|------------------------------------------------------------------------|\n",
    "| **ID number**            | Identifier for each case                                              |\n",
    "| **Diagnosis**            | M = malignant, B = benign                                             |\n",
    "| **radius**               | Mean of distances from center to points on the perimeter              |\n",
    "| **texture**              | Standard deviation of gray-scale values                               |\n",
    "| **perimeter**            | Perimeter of the contour                                              |\n",
    "| **area**                 | Area within the contour                                               |\n",
    "| **smoothness**           | Local variation in radius lengths                                     |\n",
    "| **compactness**          | Perimeter² / Area - 1.0                                               |\n",
    "| **concavity**            | Severity of concave portions of the contour                           |\n",
    "| **concave points**       | Number of concave portions of the contour                             |\n",
    "| **symmetry**             | Symmetry of the contour                                               |\n",
    "| **fractal dimension**    | \"Coastline approximation\" - 1                                         |\n",
    "\n",
    "To facilitate result comparison, we made the following improvements:\n",
    "\n",
    "- Remove the ID number column as the irrelevant column.\n",
    "- Encode the values in the Diagnosis column, where M = malignant and B = benign, into Malignant = 1 and Benign = 0.\n",
    "- Set the number of decision trees used by XGBoost to 5, the maximum depth of the trees to 5, and the learning rate to 0.3.\n",
    "\n",
    "We split the dataset into 20% training and 80% testing sets, using 5-fold cross-validation to evaluate the accuracy of our model and the previous work's model:\n",
    "\n",
    "- Previous work:\n",
    "\n",
    "![breast_cancer_diagnosis_previous_work_output](src/breast_cancer_diagnosis_previous_work_output.png)\n",
    "\n",
    "- Our work:\n",
    "\n",
    "![breast_cancer_diagnosis_our_output](src/breast_cancer_diagnosis_our_output.png)\n",
    "\n",
    "From the comparison of the two figures, it can be seen that under the 5-fold experiment, our average accuracy is 0.52% higher than the previous work. This shows that we successfully reproduced the previous work.\n",
    "\n",
    "**Customer Exited Prediction**:\n",
    "\n",
    "This work uses XGBoost to predict customer retention based on information such as credit score, age, and income. The dataset used is Churn_Predictions_Personal. This dataset includes the following features:\n",
    "\n",
    "| **Feature**        | **Description**                                                                 |\n",
    "|---------------------|---------------------------------------------------------------------------------|\n",
    "| `RowNumber`        | The row number in the dataset (used as an index).                               |\n",
    "| `CustomerId`       | A unique identifier for each customer.                                          |\n",
    "| `Surname`          | The last name of the customer.                                                  |\n",
    "| `CreditScore`      | The credit score of the customer.                                               |\n",
    "| `Geography`        | The location or country of the customer.                                        |\n",
    "| `Gender`           | The gender of the customer (e.g., Male, Female).                                |\n",
    "| `Age`              | The age of the customer.                                                        |\n",
    "| `Tenure`           | The number of years the customer has been with the provider.                    |\n",
    "| `Balance`          | The account balance of the customer.                                            |\n",
    "| `NumOfProducts`    | The number of products the customer has with the provider.                      |\n",
    "| `HasCrCard`        | Whether the customer has a credit card (1 = Yes, 0 = No).                       |\n",
    "| `IsActiveMember`   | Whether the customer is an active member (1 = Yes, 0 = No).                     |\n",
    "| `EstimatedSalary`  | The estimated salary of the customer.                                           |\n",
    "| `Exited`           | Whether the customer exited the bank (1 = Yes, 0 = No).                         |\n",
    "\n",
    "To facilitate result comparison, we made the following improvements:\n",
    "\n",
    "- Remove the RowNumber, CustomerId, and Surname columns as the irrelevant columns.\n",
    "- Encode the Gender and Geography features with the LabelEncoder\n",
    "- Set the number of decision trees used by XGBoost to 5, the maximum depth of the trees to 5, and the learning rate to 0.3.\n",
    "\n",
    "We split the dataset into 20% training and 80% testing sets, using 5-fold cross-validation to evaluate the accuracy of our model and the previous work's model:\n",
    "\n",
    "- Previous work:\n",
    "\n",
    "![customer_exited_prediction_previous_work_output](src/customer_exited_prediction_previous_work_output.png)\n",
    "\n",
    "- Our work:\n",
    "\n",
    "![customer_exited_prediction_our_output](src/customer_exited_prediction_our_output.png)\n",
    "\n",
    "From the comparison of the two figures, it can be seen that under the 5-fold experiment, our average accuracy is only 0.6% lower than the previous work. This shows that we successfully reproduced the previous work.\n",
    "\n",
    "**Titanic Survival Prediction**:\n",
    "\n",
    "This work uses XGBoost to predict which passengers would survive the Titanic shipwreck. The dataset used is Titanic - Machine Learning from Disaster. Below are the features included in the dataset:\n",
    "\n",
    "| **Feature**   | **Description**                                       |\n",
    "|---------------|-------------------------------------------------------|\n",
    "| `PassengerId` | Unique identifier for each Passenger.                 |\n",
    "| `Survival`    | Survival status (0 = No, 1 = Yes).                    |\n",
    "| `Pclass`      | Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).             |\n",
    "| `Sex`         | Sex of the passenger.                                 |\n",
    "| `Age`         | Age of the passenger in years.                        |\n",
    "| `Sibsp`       | Number of siblings/spouses aboard the Titanic.        |\n",
    "| `Parch`       | Number of parents/children aboard the Titanic.        |\n",
    "| `Ticket`      | Ticket number.                                        |\n",
    "| `Fare`        | Passenger fare.                                       |\n",
    "| `Cabin`       | Cabin number.                                         |\n",
    "| `Embarked`    | Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton). |\n",
    "\n",
    "To facilitate result comparison, we made the following improvements:\n",
    "\n",
    "- Remove the Name, PassengerId, Ticket columns as the irrelevant columns.\n",
    "- Count the null values in the Cabin and Age columns and set the number of null values into a new feature: 0 means no null values, 1 means one null value, and 2 means all values are nulls.\n",
    "- Extract the first letter of the Cabin code and map it to a number.\n",
    "- Fill the missing cells in Fare with the average ticket price of third-class passengers.\n",
    "- Fill the missing cells in Embarked with the Embarkation Southampton.\n",
    "- Convert the 'Sex', 'Nulls', 'Cabin_mapped', 'Embarked' columns into one-hot encodings.\n",
    "- Set the number of decision trees used by XGBoost to 5, the maximum depth of the trees to 5, and the learning rate to 0.3.\n",
    "\n",
    "We split the dataset into 20% training and 80% testing sets, using 5-fold cross-validation to evaluate the accuracy of our model and the previous work's model:\n",
    "\n",
    "- Previous work:\n",
    "\n",
    "![titanic_survival_prediction_previou_work_output_output](src/titanic_survival_prediction_previou_work_output.png)\n",
    "\n",
    "- Our work:\n",
    "\n",
    "![titanic_survival_prediction_our_output](src/titanic_survival_prediction_our_output.png)\n",
    "\n",
    "From the comparison of the two figures, it can be seen that under the 5-fold experiment, our average accuracy is only 0.8% lower than the previous work. This shows that we successfully reproduced the previous work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e594c1-6718-4118-a472-b384ac6ccc62",
   "metadata": {},
   "source": [
    "# Citation and Reference\n",
    "\n",
    "1. Chen, T. & Guestrin, C. (2016) 'XGBoost: A scalable tree boosting system', *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*. ACM. Available at: https://doi.org/10.1145/2939672.2939785 (Accessed: 28 November 2024).\n",
    "\n",
    "2. Friedman, J.H. (2001) 'Greedy function approximation: A gradient boosting machine', *Annals of Statistics*, 29(5), pp. 1189–1232. Available at: https://doi.org/10.1214/aos/1013203451 (Accessed: 28 November 2024).\n",
    "\n",
    "3. Hastie, T., Tibshirani, R. & Friedman, J. (2009) *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. 2nd edn. Springer Science & Business Media. Available at: https://doi.org/10.1007/978-0-387-84858-7 (Accessed: 28 November 2024).\n",
    "\n",
    "4. Bishop, C.M. (2006) *Pattern Recognition and Machine Learning*. 1st edn. Springer. Available at: https://doi.org/10.1007/978-0-387-45528-0 (Accessed: 28 November 2024).\n",
    "\n",
    "5. Quinlan, J.R. (1996) 'Bagging, boosting, and C4.5', *Proceedings of the National Conference on Artificial Intelligence*, pp. 725–730. Available at: https://www.aaai.org/Papers/AAAI/1996/AAAI96-108.pdf (Accessed: 28 November 2024).\n",
    "\n",
    "6. Breiman, L. (1996) 'Bagging predictors', *Machine Learning*, 24(2), pp. 123–140. Available at: https://doi.org/10.1007/BF00058655 (Accessed: 28 November 2024).\n",
    "\n",
    "7. Chen, T., He, T., Benesty, M., Khotilovich, V. & Tang, Y. (2020) *XGBoost: Extreme Gradient Boosting*. R Package version 1.4.1. Available at: https://xgboost.readthedocs.io/ (Accessed: 28 November 2024).\n",
    "\n",
    "8. Breast Cancer Wisconsin (Diagnostic) Data Set: Kaggle (n.d.) 'Breast Cancer Wisconsin (Diagnostic) Data Set'. Available at: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data (Accessed: 4 December 2024).\n",
    "\n",
    "9. Customer Exited Prediction | XGBoost: Ezzeldean, M. (n.d.) 'Customer Exited Prediction | XGBoost'. Kaggle. Available at: https://www.kaggle.com/code/mohammedezzeldean/customer-exited-prediction-xgboost/notebook (Accessed: 4 December 2024).\n",
    "\n",
    "10. Titanic - Machine Learning from Disaster: Kaggle (n.d.) 'Titanic - Machine Learning from Disaster'. Available at: https://www.kaggle.com/c/titanic/overview (Accessed: 4 December 2024).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f3522-f55d-4c2b-86a5-3f735ee513f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

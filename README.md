# XGBoost Algorithm Overview

XGBoost, or Extreme Gradient Boosting, is an efficient, scalable machine learning algorithm used primarily for supervised learning tasks like classification and regression. It builds upon gradient boosting principles to create an ensemble of weak learners that sequentially correct the errors of previous models to improve accuracy.

## How XGBoost Works

1. **Initialization**:
   - Starts with an initial prediction (average value for regression or a default probability for classification).

2. **Iterative Model Training**:
   - In each step, a new weak learner (we are using decision tree) is trained to minimize residual errors from previous models. 
   - The weak learner is trained on a modified dataset where the target is now the residual error from the last iteration.

3. **Gradient Boosting with Regularization**:
   
   - XGBoost includes regularization terms in the objective function to control overfitting:

     $\text{Objective} = \sum_{i=1}^{n} L(y_i, \hat{y}\_i) + \sum_{k=1}^{K} \Omega(f_k)$
     
     where $L(y_i, \hat{y}_i)$ is the loss function, and $\Omega(f_k)$ is the regularization term for tree $f_k$.
   
4. **Shrinking (Learning Rate)**:
   - Applies a learning rate to scale each weak learner’s contribution, ensuring gradual model improvement to prevent overfitting.
   
5. **Tree Pruning**:
   - Uses constraints like "max depth" to limit tree depth, preventing overfitting.
   
6. **Weighted Data and Column Sampling**:
   - Row and column sampling prevent overfitting, making the model more robust to noisy data.
   
7. **Final Prediction**:
   - Predictions are generated by aggregating the outputs of all weak learners, often by summing their outputs.

## Advantages of XGBoost

1. **Highly Efficient and Scalable**:
   - Optimized for speed, utilizing CPU/GPU resources for large datasets.

2. **Regularization**:
   - L1 and L2 regularization helps reduce overfitting, improving generalization.

3. **Custom Loss Functions**:
   - Allows custom loss functions, adapting well to various tasks and metrics.

4. **Handles Missing Values**:
   - Automatically learns the best direction for missing values during training.

5. **Parallel and Distributed Computing**:
   - Supports parallel tree boosting, and distributed training, making it suitable for very large datasets.

6. **Feature Importance and Interpretability**:
   - Provides feature importance scores for insight into feature contributions.

## Disadvantages of XGBoost

1. **Complexity in Tuning**:
   - Many hyperparameters require tuning; poor parameter settings may lead to suboptimal performance.

2. **Sensitive to Noise**:
   - Can overfit noisy data or when trees are too deep, despite regularization.

3. **High Memory Consumption**:
   - Memory-intensive on large datasets with high-dimensional data.

4. **Not Ideal for Small Datasets**:
   - On small datasets, simpler models may perform better with fewer resources.

5. **Black-box Nature**:
   - Though feature importance scores provide some interpretability, XGBoost can still be difficult to fully interpret.

## Summary

XGBoost is a powerful algorithm, ideal for large datasets and structured data tasks, but its complexity and memory requirements can make it less suitable for small datasets or when a highly interpretable model is required.

# Representation of XGBoost

In XGBoost with Decision Stump as weak learner, each feature value contributes to a single number prediction through a sequence of simple one-level trees. Here’s how XGBoost converted feature values into a single number prediction:

1. **Initialization**:

   - The model starts with an initial prediction for all samples, often set to zero or the average target value if it's a regression task. Let's denote this initial prediction as $F^{(0)}(x)$.

2. **Training Decision Stumps**:

   - In each boosting round $t$, a new Decision Stump $h_t(x)$ is trained to predict the residuals (the difference between the true values $y_i$ and the current predictions $F^{(t-1)}(x_i)$.
   - Each Decision Stump splits the data based on a single feature and threshold, creating a simple rule: if the feature value $x_i$ is less than the threshold, the stump assigns one prediction; otherwise, it assigns a different prediction.

3. **Tree Prediction**:

   - For a feature $x$ and a threshold $\theta$, the Decision Tree makes predictions by assigning a constant value to samples on each side of the threshold:
     
$$
h_t(x) = 
\begin{cases} 
      y_{\text{left}} & \text{if } x_i < \theta \\
      y_{\text{right}} & \text{otherwise}
\end{cases}
$$

   - Here, $y_{\text{left}}$ and $y_{\text{right}}$ are the average residuals for samples on each side of the threshold, chosen to minimize the overall error in predicting residuals.

4. **Updating the Overall Prediction**:

   - The model’s prediction is updated by adding a scaled version of the Decision Stump’s prediction. The learning rate $\eta$ controls how much each stump contributes to the final model: 
     $$F^{(t)}(x) = F^{(t-1)}(x) + \eta h_t(x)$$

   - This update means each Decision Stump contributes only a small correction to the existing prediction, allowing the model to make gradual adjustments rather than large changes.

5. **Final Prediction**:

   - After $T$ boosting rounds, the final prediction for a data point $x$​​ is the sum of all weak learners' contributions: 
     $$F(x) = \sum_{t=1}^T \eta h_t(x)$$

   - Each Decision Stump captures a simple pattern based on one feature and threshold, and by combining multiple stumps, XGBoost can approximate complex relationships in the data.

# Loss of XGBoost

The loss is used to measure the error between predicted and actual values. XGBoost supports various loss functions tailored to different types of tasks, including regression and classification tasks.

1. **Loss Function**:
    For the regression task, we can use Mean Squared Error(MSE) or Mean Absolute Error (MAE)

    Mean Squared Error(MSE):

    $$L(F^{\mathbf{(t)}}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - F^{\mathbf{(t)}}(\mathbf{x}_i))^2$$

    Mean Absolute Error (MAE):

    $$L(F^{\mathbf{(t)}}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - F^{\mathbf{(t)}}(\mathbf{x}_i)|$$

    For the binary classification task, we can use Binary Cross Entropy Loss
    Binary Cross Entropy Loss:

    $$L_S(F^{\mathbf{(t)}}) = -\frac{1}{n} \sum_{i=1}^{n}\left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - F^{\mathbf{(t)}}(\mathbf{x}_i)) \right]$$

    For the multiclass classification task, we can use Cross Entropy Loss
    Cross Entropy Loss:
    $$L_S(F^{\mathbf{(t)}}) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k} \mathbb{1}[y_i = j] \log F^{\mathbf{(t)}}(\mathbf{x}_i)_j$$

    where:
    
    - $y_i$ is the $i$-th actual value 

    - $n$ is the number of samples 

    - $k$ is the number of classes 
    
    - $j$ is the $j$-th class 
    
    - $F^{\mathbf{(t)}}$ is the model at the $t$-th iteration.

# XGBoost with Decision Stump as Weak Learner: Optimizer Update

In this configuration, XGBoost uses a decision stump as the weak learner. The optimizer is updated to account for the decision stump mechanism, which assigns constant values to samples based on a feature threshold, and the prediction update process incorporates a learning rate to scale the contribution of each stump.

## Objective Function

The objective function consists of the loss and regularization terms:

$$\text{Objective} = \sum_{i=1}^{n} L(y_i, \hat{y}\_i) + \sum_{k=1}^{K} \Omega(f_k)$$

where $L(y_i, \hat{y}_i)$ is the loss function, typically squared error or logistic loss, measuring the difference between the true values $y_i$ and predictions $\hat{y}_i$. $\Omega(f_k)$ is the regularization term to control model complexity.

At each iteration $t$, the model updates the prediction with the new decision stump’s prediction, scaled by the learning rate $\eta$:

$$
F^{(t)}(x) = F^{(t-1)}(x) + \eta h_t(x)
$$

where $h_t(x)$ represents the decision stump’s prediction.

## Weak Learner

For a feature $x$ and a threshold $\theta$, the decision stump assigns a constant prediction to samples based on the threshold:

$$
h_t(x) = 
\begin{cases} 
      y_{\text{left}} & \text{if } x_i < \theta \\
      y_{\text{right}} & \text{otherwise}
\end{cases}
$$

Here, $y_{\text{left}}$ and $y_{\text{right}}$ are the average residuals for samples on each side of the threshold, chosen to minimize the error in predicting residuals.

## Approximation with Taylor Expansion

To facilitate optimization, we apply a second-order Taylor expansion around the current prediction $F^{(t-1)}$ to approximate the loss function $L(F^{(t)})$:

$$
L(F^{(t)}) \approx \sum_{i=1}^{n} \left[ L(y_i, F^{(t-1)}(x_i)) + g_i h_t(x_i) + \frac{1}{2} h_i h_t(x_i)^2 \right] + \Omega(h_t)
$$

where:
- $g_i = \frac{\partial L(y_i, F^{(t-1)}(x_i))}{\partial F^{(t-1)}(x_i)}$ is the first derivative of the loss with respect to the previous prediction (the gradient).
- $h_i = \frac{\partial^2 L(y_i, F^{(t-1)}(x_i))}{\partial F^{(t-1)}(x_i)^2}$ is the second derivative (the Hessian).

## Regularization and Optimal Leaf Weights

The regularization term for a decision stump $\Omega(h_t)$ is given by:

$$
\Omega(h_t) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
$$

where:
- $T$ is the number of leaf nodes,
- $w_j$ is the weight assigned to each leaf, 
- $\gamma$ controls the complexity penalty, and $\lambda$ controls the weight shrinkage.

The optimal weight for each leaf $j$ is obtained by minimizing the regularized objective:

$$
w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}
$$

where $I_j$ is the set of sample indices for leaf $j$.

## Gain Calculation and Tree Update

The gain for adding a new stump, which represents the improvement in the objective function, is:

$$
\text{Gain} = \frac{1}{2} \sum_{j=1}^{T} \frac{\left( \sum_{i \in I_j} g_i \right)^2}{\sum_{i \in I_j} h_i + \lambda} - \gamma T
$$

This gain metric helps determine the best split points and decide whether further splitting is beneficial.

## Prediction Update

Finally, the model’s prediction is updated at each iteration with the contribution from the newly added decision stump:

$$
F^{(t)}(x) = F^{(t-1)}(x) + \eta h_t(x)
$$

where $\eta$ is the learning rate.

---

**Input**:

Training set $S = \{(x_1, y_1), \ldots, (x_m, y_m)\}$

Weak learner $WL$ (Decision Stump)

Number of boosting rounds $R$

Learning rate $\eta$

Regularization parameters $\lambda$ and $\gamma$

**Initialize**: Set initial prediction $F^{(0)}(x) = 0$

For each iteration t = 1, ..., R:

1. Compute gradients $g_i$ and Hessians $h_i$ for each sample $i = 1, \ldots, m$

2. For each feature and threshold $\theta$:

   a. Split the data into left and right groups based on $\theta$

   b. Compute gain for this split based on gradients and Hessians

3. Select feature and threshold with the highest gain

4. Fit decision stump $h_t(x)$ on the selected split:
   a. Set constant values $y_{\text{left}}$ and $y_{\text{right}}$ for the left and right groups

5. Compute optimal weights $w_j$ for each leaf node (left and right groups)

6. Update model prediction:
   $$F^{(t)}(x) = F^{(t-1)}(x) + \eta h_t(x)$$

**Output**: Final model prediction $F(x) = F^{(R)}(x)$

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f68276-a7e2-43e8-8eb0-191ccefa7ea1",
   "metadata": {},
   "source": [
    "# XGBoost Algorithm Overview\n",
    "\n",
    "XGBoost, or Extreme Gradient Boosting, is an efficient, scalable machine learning algorithm used primarily for supervised learning tasks like classification and regression. It builds upon gradient boosting principles to create an ensemble of weak learners that sequentially correct the errors of previous models to improve accuracy.\n",
    "\n",
    "## How XGBoost Works\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Starts with an initial prediction (average value for regression or a default probability for classification).\n",
    "\n",
    "2. **Iterative Model Training**:\n",
    "   - In each step, a new weak learner (we are using decision tree) is trained to minimize residual errors from previous models. \n",
    "   - The weak learner is trained on a modified dataset where the target is now the residual error from the last iteration.\n",
    "\n",
    "3. **Gradient Boosting with Regularization**:\n",
    "   \n",
    "   - XGBoost includes regularization terms in the objective function to control overfitting:\n",
    "     $$\n",
    "     \\text{Objective} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(h_k)\n",
    "     $$\n",
    "     where $L(y_i, \\hat{y}_i)$ is the loss function, $\\Omega(h_k)$ is the regularization term for tree $h_k$, $n$ is the numble of samples, and $k$ is the number of trees (Chen & Guestrin, 2016). \n",
    "   \n",
    "4. **Shrinking (Learning Rate)**:\n",
    "   - Applies a learning rate to scale each weak learner’s contribution, ensuring gradual model improvement to prevent overfitting.\n",
    "   \n",
    "5. **Tree Pruning**:\n",
    "   - Uses constraints like \"max depth\" to limit tree depth, preventing overfitting.\n",
    "   \n",
    "6. **Weighted Data and Column Sampling**:\n",
    "   - Row and column sampling prevent overfitting, making the model more robust to noisy data.\n",
    "   \n",
    "7. **Final Prediction**:\n",
    "   - Predictions are generated by aggregating the outputs of all weak learners, often by summing their outputs.\n",
    "\n",
    "## Advantages of XGBoost\n",
    "\n",
    "1. **Highly Efficient and Scalable**:\n",
    "   - Optimized for speed, utilizing CPU/GPU resources for large datasets.\n",
    "\n",
    "2. **Regularization**:\n",
    "   - L1 and L2 regularization helps reduce overfitting, improving generalization (Friedman, 2001).\n",
    "\n",
    "3. **Custom Loss Functions**:\n",
    "   - Allows custom loss functions, adapting well to various tasks and metrics.\n",
    "\n",
    "4. **Handles Missing Values**:\n",
    "   - Automatically learns the best direction for missing values during training.\n",
    "\n",
    "5. **Parallel and Distributed Computing**:\n",
    "   - Supports parallel tree boosting, and distributed training, making it suitable for very large datasets.\n",
    "\n",
    "6. **Feature Importance and Interpretability**:\n",
    "   - Provides feature importance scores for insight into feature contributions.\n",
    "\n",
    "## Disadvantages of XGBoost\n",
    "\n",
    "1. **Complexity in Tuning**:\n",
    "   - Many hyperparameters require tuning; poor parameter settings may lead to suboptimal performance.\n",
    "\n",
    "2. **Sensitive to Noise**:\n",
    "   - Can overfit noisy data or when trees are too deep, despite regularization.\n",
    "\n",
    "3. **High Memory Consumption**:\n",
    "   - Memory-intensive on large datasets with high-dimensional data.\n",
    "\n",
    "4. **Not Ideal for Small Datasets**:\n",
    "   - On small datasets, simpler models may perform better with fewer resources.\n",
    "\n",
    "5. **Black-box Nature**:\n",
    "   - Though feature importance scores provide some interpretability, XGBoost can still be difficult to fully interpret.\n",
    "\n",
    "## Summary\n",
    "\n",
    "XGBoost is a powerful algorithm, ideal for large datasets and structured data tasks, but its complexity and memory requirements can make it less suitable for small datasets or when a highly interpretable model is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b176e44a-b13e-4ebd-bb2f-e1162675dc44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Representation of XGBoost\n",
    "\n",
    "In XGBoost with Decision Tree as the weak learner, predictions are made by combining the outputs of a sequence of decision trees. Here’s how XGBoost generates a single prediction from feature values:\n",
    "\n",
    "1. **Initialization**:\n",
    "\n",
    "   - The model starts with an initial prediction for all samples, often set to zero or the average target value if it's a regression task. Let's denote this initial prediction as $F^{(0)}(x)$.\n",
    "\n",
    "2. **Training Decision Trees**:\n",
    "\n",
    "   - In each boosting round $t$, a new Decision Tree $h_t(x)$ is trained to predict the residuals (the difference between the true values $y_i$ and the current predictions $F^{(t-1)}(x_i)$.\n",
    "   - At each split, the Decision Tree splits the data based on a single feature and threshold, recursively creating a set of rules. The final prediction for each sample is determined by the leaf node it falls into after traversing the tree.\n",
    "\n",
    "3. **Tree Prediction**:\n",
    "\n",
    "   - For a feature $x$ and a threshold $\\theta$, a single split in the Decision Tree assigns predictions to samples based on the threshold: \n",
    "     $$\n",
    "     h_t(x) = \\begin{cases} \n",
    "           y_{\\text{left}} & \\text{if } x_i < \\theta \\\\\n",
    "           y_{\\text{right}} & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "     $$\n",
    "\n",
    "   - Here, $y_{\\text{left}}$ and $y_{\\text{right}}$ represent the predicted values for the samples on each side of the split. These predictions are often chosen to minimize the overall error in the objective function (Hastie, Tibshirani & Friedman, 2009).\n",
    "\n",
    "4. **Updating the Overall Prediction**:\n",
    "\n",
    "   - The model’s prediction is updated by adding a scaled version of the Decision Tree’s prediction. The learning rate $\\eta$ controls how much each tree contributes to the final model: \n",
    "     $$\n",
    "     F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)\n",
    "     $$\n",
    "\n",
    "   - This update means each Decision Tree contributes only a small correction to the existing prediction, allowing the model to make gradual adjustments rather than large changes.\n",
    "\n",
    "5. **Final Prediction**:\n",
    "\n",
    "   - After $T$ boosting rounds, the final prediction for a data point $x$ is the sum of all weak learners' contributions: \n",
    "     $$\n",
    "     F(x) = \\sum_{t=1}^T \\eta h_t(x)\n",
    "     $$\n",
    "\n",
    "   - Each Decision Tree captures patterns by recursively splitting data based on features and thresholds. By combining multiple trees, XGBoost can approximate complex relationships in the data (Chen & Guestrin, 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c26193-0857-4fd3-a921-636b20bc8ad5",
   "metadata": {},
   "source": [
    "# Loss of XGBoost\n",
    "\n",
    "The loss is used to measure the error between predicted and actual values. XGBoost supports various loss functions tailored to different types of tasks, including regression and classification tasks.\n",
    "\n",
    "1. **Loss Function**:\n",
    "    For the regression task, we can use Mean Squared Error(MSE) or Mean Absolute Error (MAE) (Hastie, Tibshirani & Friedman, 2009)\n",
    "\n",
    "    Mean Squared Error(MSE):\n",
    "    $$\n",
    "        L(F^{\\mathbf{(t)}}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - F^{\\mathbf{(t)}}(\\mathbf{x}_i))^2\n",
    "    $$\n",
    "\n",
    "    Mean Absolute Error (MAE):\n",
    "    $$\n",
    "        L(F^{\\mathbf{(t)}}) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - F^{\\mathbf{(t)}}(\\mathbf{x}_i)|\n",
    "    $$\n",
    "    \n",
    "    For the binary classification task, we can use Binary Cross Entropy Loss\n",
    "    Binary Cross Entropy Loss:\n",
    "\n",
    "    $$\n",
    "    L_S(F^{\\mathbf{(t)}}) = -\\frac{1}{n} \\sum_{i=1}^{n}\\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - F^{\\mathbf{(t)}}(\\mathbf{x}_i)) \\right]\n",
    "    $$\n",
    "\n",
    "    For the multiclass classification task, we can use Cross Entropy Loss\n",
    "    Cross Entropy Loss (Bishop, 2006):\n",
    "    $$\n",
    "        L_S(F^{\\mathbf{(t)}}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} \\mathbb{1}[y_i = j] \\log F^{\\mathbf{(t)}}(\\mathbf{x}_i)_j\n",
    "    $$\n",
    "\n",
    "    where:\n",
    "    \n",
    "    - $y_i$ is the $i$-th actual value \n",
    "\n",
    "    - $n$ is the number of samples \n",
    "\n",
    "    - $k$ is the number of classes \n",
    "    \n",
    "    - $j$ is the $j$-th class \n",
    "    \n",
    "    - $F^{\\mathbf{(t)}}$ is the model at the $t$-th iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ecff9-3ad4-4921-b638-01288b993ea7",
   "metadata": {},
   "source": [
    "# XGBoost with Decision Tree as Weak Learner: Optimizer Update\n",
    "\n",
    "In this configuration, XGBoost uses a decision tree as the weak learner. The optimizer is updated to account for the decision tree mechanism, which recursively splits the data based on features and thresholds. Each leaf node assigns a constant value to the samples it contains. The prediction update process incorporates a learning rate $\\eta$ to scale the contribution of each tree, ensuring gradual and controlled adjustments to the model (Quinlan, 1996).\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "The objective function consists of the loss and regularization terms:\n",
    "\n",
    "$$\n",
    "\\text{Objective} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(h_k)\n",
    "$$\n",
    "\n",
    "where $L(y_i, \\hat{y}_i)$ is the loss function, typically squared error or logistic loss, measuring the difference between the true values $y_i$ and predictions $\\hat{y}_i$. $\\Omega(h_k)$ is the regularization term to control model complexity.\n",
    "\n",
    "At each iteration $t$, the model updates the prediction with the new decision tree’s prediction, scaled by the learning rate $\\eta$:\n",
    "\n",
    "$$\n",
    "F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)\n",
    "$$\n",
    "\n",
    "where $h_t(x)$ represents the decision tree’s prediction.\n",
    "\n",
    "## Weak Learner\n",
    "\n",
    "For a feature $x$ and a threshold $\\theta$, a single split in the Decision Tree assigns predictions to samples based on the threshold: \n",
    "     $$\n",
    "     h_t(x) = \\begin{cases} \n",
    "           y_{\\text{left}} & \\text{if } x_i < \\theta \\\\\n",
    "           y_{\\text{right}} & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "     $$\n",
    "\n",
    "Here, $y_{\\text{left}}$ and $y_{\\text{right}}$ represent the predicted values for the samples on each side of the split. These predictions are often chosen to minimize the overall error in the objective function.\n",
    "\n",
    "## Approximation with Taylor Expansion\n",
    "\n",
    "To facilitate optimization, we apply a second-order Taylor expansion around the current prediction $F^{(t-1)}$ to approximate the loss function $L(F^{(t)})$:\n",
    "\n",
    "$$\n",
    "L(F^{(t)}) \\approx \\sum_{i=1}^{n} \\left[ L(y_i, F^{(t-1)}(x_i)) + g_i h_t(x_i) + \\frac{1}{2} h_i h_t(x_i)^2 \\right] + \\Omega(h_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $g_i = \\frac{\\partial L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)}$ is the first derivative of the loss with respect to the previous prediction (the gradient).\n",
    "- $h_i = \\frac{\\partial^2 L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)^2}$ is the second derivative (the Hessian).\n",
    "\n",
    "## Regularization and Optimal Leaf Weights\n",
    "\n",
    "The regularization term for a decision tree $\\Omega(h_t)$ is given by:\n",
    "\n",
    "$$\n",
    "\\Omega(h_t) = \\gamma N + \\frac{1}{2} \\lambda \\sum_{j=1}^{N} w_j^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of leaf nodes,\n",
    "- $w_j$ is the weight assigned to each leaf, \n",
    "- $\\gamma$ controls the complexity penalty, and $\\lambda$ controls the weight shrinkage.\n",
    "\n",
    "The optimal weight for each leaf $j$ is obtained by minimizing the regularized objective:\n",
    "\n",
    "$$\n",
    "w_j^* = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}\n",
    "$$\n",
    "\n",
    "where $I_j$ is the set of sample indices for leaf $j$.\n",
    "\n",
    "## Gain Calculation and Tree Update\n",
    "\n",
    "The gain for adding a new tree, which represents the improvement in the objective function, is:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\sum_{j=1}^{N} \\frac{\\left( \\sum_{i \\in I_j} g_i \\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} - \\gamma N\n",
    "$$\n",
    "\n",
    "This gain metric helps determine the best split points and decide whether further splitting is beneficial.\n",
    "\n",
    "## Prediction Update\n",
    "\n",
    "Finally, the model’s prediction is updated at each iteration with the contribution from the newly added decision tree:\n",
    "\n",
    "$$\n",
    "F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a102f-e8ea-4b9d-9281-7b64796f6a22",
   "metadata": {},
   "source": [
    "# XGBoost Pseudo-code\n",
    "\n",
    "**Input**:  \n",
    "- Training set $S = \\{(x_1, y_1), \\ldots, (x_m, y_m)\\}$  \n",
    "- Weak learner $h_t(x)$ (Decision Tree)  \n",
    "- Number of boosting rounds $T$  \n",
    "- Learning rate $\\eta$  \n",
    "- Regularization parameters $\\lambda, \\gamma$  \n",
    "\n",
    "**Initialize**:  \n",
    "$F^{(0)}(x) = 0$  \n",
    "\n",
    "**for** $t = 1, \\ldots, T$:  \n",
    "1. **Compute gradients and Hessians**:  \n",
    "   $g_i = \\frac{\\partial L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)}$  \n",
    "   $h_i = \\frac{\\partial^2 L(y_i, F^{(t-1)}(x_i))}{\\partial F^{(t-1)}(x_i)^2}$  \n",
    "\n",
    "2. **Find the best split**:  \n",
    "   - For each feature and threshold $\\theta$:  \n",
    "     - Split data into left and right groups based on $\\theta$  \n",
    "     - Compute split gain using gradients and Hessians  \n",
    "   - Select feature and threshold $\\theta$ with the highest gain  \n",
    "\n",
    "3. **Train decision tree** $h_t(x)$:  \n",
    "   - Fit $h_t(x)$ using the selected splits  \n",
    "   - Assign values $y_{\\text{left}}$ and $y_{\\text{right}}$ to leaf nodes  \n",
    "   - Compute optimal weights $w_j$ for each leaf node  \n",
    "\n",
    "4. **Update model**:  \n",
    "   $F^{(t)}(x) = F^{(t-1)}(x) + \\eta h_t(x)$  \n",
    "\n",
    "**Output**:  \n",
    "Final model prediction: $F(x) = F^{(T)}(x)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e594c1-6718-4118-a472-b384ac6ccc62",
   "metadata": {},
   "source": [
    "# Citation and Reference\n",
    "\n",
    "1. Chen, T. & Guestrin, C. (2016) 'XGBoost: A scalable tree boosting system', *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*. ACM. Available at: https://doi.org/10.1145/2939672.2939785 (Accessed: 28 November 2024).\n",
    "\n",
    "2. Friedman, J.H. (2001) 'Greedy function approximation: A gradient boosting machine', *Annals of Statistics*, 29(5), pp. 1189–1232. Available at: https://doi.org/10.1214/aos/1013203451 (Accessed: 28 November 2024).\n",
    "\n",
    "3. Hastie, T., Tibshirani, R. & Friedman, J. (2009) *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. 2nd edn. Springer Science & Business Media. Available at: https://doi.org/10.1007/978-0-387-84858-7 (Accessed: 28 November 2024).\n",
    "\n",
    "4. Bishop, C.M. (2006) *Pattern Recognition and Machine Learning*. 1st edn. Springer. Available at: https://doi.org/10.1007/978-0-387-45528-0 (Accessed: 28 November 2024).\n",
    "\n",
    "5. Quinlan, J.R. (1996) 'Bagging, boosting, and C4.5', *Proceedings of the National Conference on Artificial Intelligence*, pp. 725–730. Available at: https://www.aaai.org/Papers/AAAI/1996/AAAI96-108.pdf (Accessed: 28 November 2024).\n",
    "\n",
    "6. Breiman, L. (1996) 'Bagging predictors', *Machine Learning*, 24(2), pp. 123–140. Available at: https://doi.org/10.1007/BF00058655 (Accessed: 28 November 2024).\n",
    "\n",
    "7. Chen, T., He, T., Benesty, M., Khotilovich, V. & Tang, Y. (2020) *XGBoost: Extreme Gradient Boosting*. R Package version 1.4.1. Available at: https://xgboost.readthedocs.io/ (Accessed: 28 November 2024).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f3522-f55d-4c2b-86a5-3f735ee513f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
